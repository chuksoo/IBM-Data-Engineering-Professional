---
title: "ETL and Data Pipelines with Shell, Airflow and Kafka Notes"
author: "Chuks Okoli"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    latex_engine: xelatex
  html_document:
    df_print: paged
  word_document: default
header-includes: \usepackage{fvextra} 
                  \usepackage{hyperref} 
                  \usepackage{caption} 
                  \usepackage{placeins}
                  \usepackage{paralist}
                  \usepackage{amsmath} 
                  \usepackage{subfig}
                  \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
editor_options:
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.width=7, fig.height=5, echo = TRUE)
```

# Week 1

## ETL using Shell Scripts

```{=latex}
\textbf{ETL and ELT Processes} \\
ETL stands for Extract, Transform, and Load. ETL is an automated data pipeline engineering methodology, whereby data is acquired and prepared for subsequent use in an analytics environment, such as a data warehouse or data mart. ETL refers to the process of curating data from multiple sources, conforming it to a unified data format or structure, and then loading the transformed data into its new environment. 
\begin{compactitem}
\item E = Extraction: Extraction process obtains or reads the data from one or more sources
  \begin{compactitem}
    \item Some common methods include: Web scraping, where data is extracted from web pages using applications such as Python or R to parse the underlying HTML code
    \item Using APIs to programmatically connect to data and query it
  \end{compactitem}
\item T = Transformation: Transformation process wrangles the data into a format that is suitable for its destination and its intended use. Transformation is done in an intermediate working environment called a ``staging area" and can include any of the following kinds of processes: 
  \begin{compactitem}
    \item Cleaning: fixing errors or missing values
    \item Filtering: selecting only what is needed 
    \item Joining disparate data sources: merging related data
    \item Normalizing: converting data to common units  
    \item Data Structuring: converting one data format to another, such as JSON, XML, or CSV to database tables 
    \item Feature engineering: such as creating KPIs for dashboards or machine learning
    \item Anonymizing and Encrypting: ensuring privacy and security 
    \item Sorting: ordering the data to improve search performance 
    \item Aggregating: summarizing granular data 
    \item Formatting and data typing: making the data compatible with its destination. 
  \end{compactitem}
\item L = Load: Loading takes the transformed data and loads it into its new environment (.e.g., database, data warehouses or data mart), ready for visualization, exploration, further transformation, and modelling
  \begin{compactitem}
    \item Data loading techniques
      \begin{compactitem}
        \item Full
        \item Incremental
        \item Scheduled
        \item On-demand
        \item Batch and stream
        \item Push and pull
        \item Parallel and serial
      \end{compactitem}
  \end{compactitem}
\end{compactitem}

The ETL process is shown in \textbf{Fig. \ref{fig:etl}}.
\begin{figure}[h!]
\centering
  \includegraphics[width=\linewidth]{ETL.png}
  \caption{The ETL process}
  \label{fig:etl}
\end{figure}
\FloatBarrier

\textbf{Summary of ETL and ELT Processes}
\begin{compactitem}
\item ETL stands for Extract, Transform, and Load 
\item Loading means writing the data to its destination environment 
\item Cloud platforms are enabling ELT to become an emerging trend 
\item The key differences between ETL and ELT include the place of transformation, flexibility, Big Data support, and time-to-insight 
\item There is an increasing demand for access to raw data that drives the evolution from ETL, which is still used, to ELT, which enables ad-hoc, self-serve analytics 
\item Data extraction often involves advanced technology including database querying, web scraping, and APIs  
\item Data transformation, such as typing, structuring, normalizing, aggregating, and cleaning, is about formatting data to suit the application 
\item Information can be lost in transformation processes through filtering and aggregation 
\item Data loading techniques include scheduled, on-demand, and incremental 
\item Data can be loaded in batches or streamed continuously 
\end{compactitem}
```

\clearpage
# Week 2

## ETL Workflows as Data Pipelines

```{=latex}
Traditionally, the overall accuracy of the ETL workflow has been a more important requirement than speed, although efficiency is usually an important factor in minimizing resource costs. To boost efficiency, data is fed through a \emph{data pipeline} in smaller packets (see \textbf{Fig. \ref{fig:etl_workflows}}). While one packet is being extracted, an earlier packet is being transformed, and another is being loaded. In this way, data can keep moving through the workflow without interruption. Any remaining bottlenecks within the pipeline can often be handled by parallelizing slower tasks.  

\begin{figure}[h!]
\centering
  \includegraphics[width=\linewidth]{ETL Workflows.png}
  \caption{ETL Workflows as Data Pipelines }
  \label{fig:etl_workflows}
\end{figure}

With conventional ETL pipelines, data is processed in \emph{batches}, usually on a repeating schedule that ranges from hours to days apart. For example, records accumulating in an Online Transaction Processing System (OLTP) can be moved as a daily batch process to one or more Online Analytics Processing (OLAP) systems where subsequent analysis of large volumes of historical data is carried out. 
```

## Staging Areas
```{=latex}
ETL pipelines are frequently used to integrate data from disparate and usually siloed systems within the enterprise. These systems can be from different vendors, locations, and divisions of the company, which can add significant operational complexity. As an example, (see \textbf{Fig. \ref{fig:staging}}) a cost accounting OLAP system might retrieve data from distinct OLTP systems utilized by the separate payroll, sales, and purchasing departments.

ETL pipelines are frequently used to integrate data from disparate and usually \emph{siloed} systems within the enterprise. These systems can be from different vendors, locations, and divisions of the company, which can add significant operational complexity. As an example, (see \textbf{Fig. \ref{fig:staging}}) a cost accounting OLAP system might retrieve data from distinct OLTP systems utilized by the separate payroll, sales, and purchasing departments. 

\begin{figure}[h!]
\centering
  \includegraphics[width=\linewidth]{Staging.png}
  \caption{An ETL data integration pipeline concept for a Cost Accounting OLAP}
  \label{fig:staging}
\end{figure}
\FloatBarrier
```

## ETL Workflows as DAGs
```{=latex}
ETL workflows can involve considerable complexity. By breaking down the details of the workflow into individual tasks and dependencies between those tasks, one can gain better control over that complexity. Workflow orchestration tools such as Apache Airflow do just that.

Airflow represents your workflow as a directed acyclic graph (DAG). A simple example of an Airflow DAG is illustrated in \textbf{Fig. \ref{fig:dags}}. Airflow tasks can be expressed using predefined templates, called operators. Popular operators include Bash operators, for running Bash code, and Python operators for running Python code, which makes them extremely versatile for deploying ETL pipelines and many other kinds of workflows into production. 

\begin{figure}[h!]
\centering
  \includegraphics[width=\linewidth]{DAGs.png}
  \caption{An Apache Airflow DAG representing a workflow}
  \label{fig:dags}
\end{figure}
\FloatBarrier
```

## An Introduction to Data Pipelines

```{=latex}
\textbf{What is a data pipeline} \\
Data pipelines are pipelines that specifically move or modify data. The purpose of a data pipeline is to move data from one place or form to another. A data pipeline is a system which extracts data and passes it along to optional transformation stages for final loading. This includes low-level hardware architectures, but our focus here is on data pipelines as architectures driven by software processes, such as commands, programs, and processing threads. The simple Bash pipe command in Linux can be used as the 'glue' that connects such processes together.

We can think of data flowing through a pipeline in the form of data packets (see \textbf{Fig. \ref{fig:packet_pipeline}}), a term which we will use to broadly refer to units of data. Packets can range in size from a single record, or event, to large collections of data. Here, we have data packets queued for ingestion to the pipeline. The length of the data pipeline represents the time it takes for a single packet to traverse the pipeline. The arrows between packets represent the throughput delays, or the times between successive packet arrivals. 

\begin{figure}[h!]
\centering
  \includegraphics[width=0.75\linewidth]{Packet-through-pipeline.png}
  \caption{Packet flow through a pipeline}
  \label{fig:packet_pipeline}
\end{figure}
\FloatBarrier

There are two key performance considerations regarding data pipelines. The first is \emph{latency}, which is the total time it takes for a single packet of data to pass through the pipeline as shown in \textbf{Fig. \ref{fig:pipeline_latency}}. Equivalently, latency is the sum of the individual times spent during each processing stage within the pipeline. Thus overall latency is limited by the slowest process in the pipeline. For example, no matter how fast your internet service is, the time it takes to load a web page will be decided by the server's speed. The second performance consideration is called \emph{throughput}. It refers to how much data can be fed through the pipeline per unit of time. Processing larger packets per unit of time increases throughput. 

Some of the applications of data pipelines from the multitude of use cases includes: 
\begin{compactitem}
  \item The simplest pipeline is one which has no transformations and is used to copy data from one location to another, as in file backups. 
  \item Integrating disparate raw data sources into a data lake. 
  \item Moving transactional records to a data warehouse. 
  \item Streaming data from IoT devices to make information available in the form of dashboards or alerting systems. 
  \item Preparing raw data for machine learning development or production.
  \item Message sending and receiving, such as with email, SMS, or online video meetings.
\end{compactitem}

\begin{figure}[h!]
\centering
  \includegraphics[width=0.75\linewidth]{Pipeline_latency.png}
  \caption{Data pipeline performance: latency}
  \label{fig:pipeline_latency}
\end{figure}
\FloatBarrier

\textbf{Key Data Pipeline Processes} \\
Data pipeline processes typically have the following stages in common as shown in \textbf{Fig. \ref{fig:pipeline-process}}: 
\begin{compactitem}
  \item Extraction of data from one or more data sources. 
  \item Ingestion of the extracted data into the pipeline. 
  \item Optional data transformation stages within the pipeline
  \item Loading of the data into a destination facility. 
  \item A mechanism for scheduling or triggering jobs to run. 
  \item Monitoring the entire workflow, and Maintenance and optimization as required to keep the pipeline up and running smoothly. 
\end{compactitem}

\begin{figure}%
    \centering
    \subfloat[\centering Stages of data pipeline]{{\includegraphics[width=0.45\linewidth]{Pipeline-process-1} }}%
    \quad
    \subfloat[\centering Further stages]{{\includegraphics[width=0.45\linewidth]{Pipeline-process-2} }}%
    \caption{Data Pipeline Process}%
    \label{fig:pipeline-process}%
\end{figure}

The data pipeline needs to be monitored (see \textbf{Fig. \ref{fig:pipeline_monitoring}}) once it is in production to ensure data integrity. Some key monitoring considerations include: 
\begin{compactitem}
  \item Latency, or the time it takes for data packets to flow through the pipeline. 
  \item Throughput demand, the volume of data passing through the pipeline over time. 
  \item Errors and failures, caused by factors such as network overloading, and failures at the source or destination systems. 
  \item Utilization rate, or how fully the pipeline's resources are being utilized, which affects cost. 
  \item Lastly, the pipeline should also have a system for logging events and alerting administrators when failures occur.
\end{compactitem}  

\begin{figure}[h!]
\centering
  \includegraphics[width=0.75\linewidth]{Pipeline_monitoring.png}
  \caption{Data pipeline monitoring considerations}
  \label{fig:pipeline_monitoring}
\end{figure}
\FloatBarrier 
  
\textbf{Handling unbalanced loads}
\begin{compactitem}
  \item Pipelines typically contain bottlenecks
  \item Slower stages may be parallelized to speed up throughput
  \item Processes can be replicated on multiple CPUs/cores/threads
  \item Data packets are then distributed across these channels
  \item Pipelines that incorporate parallelism are referred to as being dynamic or non-linear, as opposed to “static,” which applies to serial pipelines
\end{compactitem}  

\textbf{Stage synchronization}
\begin{compactitem}
  \item I/O buffers can help synchronize stages
  \item An I/O buffer is a holding area for data, placed between processing stages having different or varying delays
  \item Buffers regulate the flow of data, may improve throughput
  \item I/O buffers used to distribute loads on parallelized stages
\end{compactitem}  
  
\textbf{Summary of Introduction to Data Pipelines}
\begin{compactitem}
\item Data pipelines move data from one place, or form, to another 
\item Data flows through pipelines as a series of data packets 
\item Latency and throughput are key design considerations for data pipelines 
\item Data pipeline processes include scheduling or triggering, monitoring, maintenance, and optimization 
\item Parallelization and I/O buffers can help mitigate bottlenecks 
\item Batch pipelines extract and operate on batches of data  
\item Batch processing applies when accuracy is critical, or the most recent data isn’t required 
\item Streaming data pipelines ingest data packets one-by-one in rapid succession 
\item Streaming pipelines apply when the most current data is needed 
\item Micro-batch processing can achieve near-real-time data streaming
\item Lambda architecture applies when access to earlier data is required, and speed is important
\item Examples of streaming data pipelines use cases, such as social media feeds, fraud detection, and real-time product pricing 
\item Modern data pipeline technologies include schema and transformation support, drag-and-drop GUIs, and security features 
\item Stream-processing technologies include Apache Kafka, IBM Streams, and SQLStream 
\end{compactitem}

```

# Week 3

## Building Data Pipelines using Apache Airflow

```{=latex}
\textbf{What is Apache Airflow} 
\begin{compactitem}
  \item Great open source workflow orchestration tool
  \item A platform that lets you build and run workflows
  \item A workflow is represented as a DAG
  \item Apache Airflow is not a data streaming solution but a workflow manager
\end{compactitem} 

\begin{figure}[h!]
\centering
  \includegraphics[width=0.75\linewidth]{Airflow.png}
  \caption{Simplified view of Airflow's architecture}
  \label{fig:airflow}
\end{figure}
\FloatBarrier 
  
\textbf{Summary of Using Apache Airflow to build Data Pipelines}
\begin{compactitem}
\item Apache Airflow is scalable, dynamic, extensible, and lean 
\item The five main features of Apache Airflow are pure Python, useful UI, integration, easy to use, and open source  
\item A common use case is that Apache Airflow defines and organizes machine learning pipeline dependencies 
\item Tasks are created with Airflow operators 
\item Pipelines are specified as dependencies between tasks 
\item Pipeline DAGs defined as code are more maintainable, testable, and collaborative  
\item Apache Airflow has a rich UI that simplifies working with data pipelines 
\item You can visualize your DAG in graph or tree mode 
\item Key components of a DAG definition file include DAG arguments, DAG and task definitions, and the task pipeline 
\item The ‘schedule\_interval’ parameter specifies how often to re-run your DAG 
\item You can save Airflow logs into local file systems and send them to cloud storage, search engines, and log analyzers 
\item Airflow recommends sending production deployment logs to be analyzed by Elasticsearch or Splunk 
\item With Airflow’s UI, you can view DAGs and task events 
\item The three types of Airflow metrics are counters, gauges, and timers 
\item Airflow recommends that production deployment metrics be sent to and analyzed by Prometheus via StatsD 
\end{compactitem}



```

# Week 4

## Building Streaming Pipelines using Apache Kafka


```{=latex}

```


```{=latex}
\textbf{ETL T} \\
\textbf{}
\begin{compactitem}
\item 
\item 
\item 
\item 
\end{compactitem}
```