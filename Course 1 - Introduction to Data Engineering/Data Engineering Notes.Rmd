---
title: "Data Engineering Notes"
author: "Chuks Okoli"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    latex_engine: xelatex
  html_document:
    df_print: paged
  word_document: default
header-includes: \usepackage{fvextra} 
                  \usepackage{hyperref} 
                  \usepackage{caption} 
                  \usepackage{placeins}
                  \usepackage{paralist}
                  \usepackage{amsmath} 
                  \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
editor_options:
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.width=7, fig.height=5, echo = TRUE)
```

## Introduction to Data Engineering

```{=latex}
 In the simplest possible terms, the field of Data Engineering concerns itself with the mechanics for the flow and access of data. And its goal is to make quality data available for fact-finding and data-driven decision making.

The field of Data Engineering involves:
\begin{enumerate}
\item Collecting source data 
    \begin{itemize}
      \item Extracting, integrating, and organizing data from disparate sources
        \begin{compactitem}
          \item Develop tools, workflows, and processes that help data acquisition from multiple sources
          \item Design, build, and maintain scalable data architecture for storing source data
        \end{compactitem}
    \end{itemize}

\item Processing data
    \begin{itemize}
      \item Cleaning, transforming, and preparing data to make it usable
        \begin{compactitem}
          \item Implement and maintain distributed systems for large-scale data processing
          \item Design pipelines for the extraction, transformation, and loading of data into data repositories
          \item Design solutions for validating and safeguarding quality, privacy, and security of data
          \item Performance optimization
          \item Adherence to compliance guidelines
        \end{compactitem}
    \end{itemize}
    
\item Storing data
    \begin{itemize}
      \item Storing data for reliability and easy availability of data
        \begin{compactitem}
          \item Data stores for storage of processed data
          \item Scalable systems
          \item Ensuring data privacy, security, compliance, monitoring, backup, and recovery
        \end{compactitem}
    \end{itemize}
    
\item Making data available to users securely
  \begin{compactitem}
    \item APIs, services, and programs for retrieving data for end-users
    \item User access through interfaces and dashboards
    \item Checks and balances to ensure data security
  \end{compactitem}
\end{enumerate}

Modern data ecosystem includes a network of interconnected and continually evolving entities that include:
\begin{compactitem}
\item Data, that is available in a host of different formats, structures, and sources. 
\item Enterprise Data Environment, in which raw data is staged so it can be organized, cleaned, and optimized for use by end-users.
\item End-users, such as business stakeholders, analysts, and programmers who consume data for various purposes.
\end{compactitem}
Emerging technologies such as Cloud Computing, Machine Learning, and Big Data, are continually reshaping the data ecosystem and the possibilities it offers.

Data Engineers, Data Analysts, Data Scientists, Business Analysts, and Business Intelligence Analysts, all play a vital role in the ecosystem for deriving insights and business results from data. \\

\textbf{Responsibility of a Data Engineer}
\begin{compactitem}
\item Extract, organize, and integrate data from disparate sources
\item Prepare data for analysis and reporting by transforming and cleansing it
\item Design and manage data pipelines that encompasses the journey of data from source to destination systems
\item Setup and manage infrastructure for ingestion, processing and storage of data including data platforms, data stores, distributed systems, and data repositories.
\end{compactitem}

\textbf{Skillset of a Data Engineer} \\
Some of the technical skills includes knowledge of:
\begin{compactitem}
\item Operating Systems
  \begin{compactitem}
  \item UNIX
  \item Linus
  \item Windows Administrative Tools
  \item System Utilities \& Commands
  \end{compactitem}
\item Infrastructure Components
  \begin{compactitem}
  \item Virtual Machines
  \item Networking
  \item Application Services
  \item Cloud-based Services like AWS, Google Cloud Platform (GCP), IBM Cloud, Microsoft Azure 
  \end{compactitem}
\item Databases and Data Warehouses
  \begin{compactitem}
  \item RDBMS - IBM DB2, MySQL, Oracle Database, PostgreSQL
  \item NoSQL - Redis, MongoDB, Cassandra, Neo4J
  \item Data Warehouses - Oracle Exadata, IBM DB2 Warehouse on cloud, IBM Netezza Performance Server, Amazon Redshift
  \end{compactitem}
\item Data Pipelines
  \begin{compactitem}
  \item Apache Beam
  \item Airflow
  \item DataFlow
  \end{compactitem}
\item ETL Tools
  \begin{compactitem}
  \item IBM Infosphere Information Server
  \item AWS Glue
  \item Improvado
  \end{compactitem}
\item Proficiency in languages for querying, manipulating, and processing data
  \begin{compactitem}
  \item SQL for relational databases, SQL-like query languages for NoSQL databases
  \item Python, R, and Java
  \item Shell and Scripting languages such as Unix/Linux Shell and PowerShell
  \end{compactitem}
\item Big Data processing tools
  \begin{compactitem}
  \item Hadoop
  \item Hive
  \item Spark
  \end{compactitem}
\end{compactitem}

\textbf{Role of a Data Engineer} \\
The goal of Data Engineering is to make quality data available for analytics and decision-making. And it does this by collecting raw source data, processing data so it becomes usable, storing data, and making quality data available to users securely.  

The role of a Data Engineer includes:
\begin{compactitem}
\item Gathering data from disparate sources.
\item Integrating data into a unified view for data consumers.
\item Preparing data for analytics and reporting.
\item Managing data pipelines for a continuous flow of data from source to destination systems.
\item Managing the complete infrastructure for the collection, processing, and storage of data.
\end{compactitem}

To be successful in their role, Data Engineers need a mix of technical, functional, and soft skills.
\begin{compactitem}
\item Technical Skills include working with different operating systems and infrastructure components such as virtual machines, networks, and application services. It also includes working with databases and data warehouses, data pipelines, ETL tools, big data processing tools, and languages for querying, manipulating, and processing data. 
\item An understanding of the potential application of data in business is an important skill for a data engineer. Other functional skills include the ability to convert business requirements into technical specifications, an understanding of the software development lifecycle, and the areas of data quality, privacy, security, and governance. 
\item Soft Skills include interpersonal skills, the ability to work collaboratively, teamwork, and effective communication. 
\end{compactitem}
```

## Data Ecosystems

```{=latex}
A Data Engineerâ€™s ecosystem includes the infrastructure, tools, frameworks, and processes for extracting data, architecting and managing data pipelines and data repositories, managing workflows, developing applications, and managing BI and Reporting tools.

There are two main types of data repositories - Transactional and analytical.
\begin{compactitem}
\item \textbf{Transactional systems, also known as Online Transaction Processing (or OLTP) systems}, are designed to store high-volume day-to-day operational data. Such as online banking transactions, ATM transactions, and airline bookings. While OLTP databases are typically relational, they can also be non-relational. 
\item \textbf{Analytical systems, also known as Online Analytical Processing (OLAP) systems}, are optimized for conducting complex data analytics. These include relational and non-relational databases, data warehouses, data marts, data lakes, and big data stores. The type, format, sources of data, and context of use influence which data repository is ideal.
\end{compactitem}

Based on how well-defined the structure of the data is, data can be categorized as
\begin{compactitem}
\item Structured data, that is data which is well organized in formats that can be stored in databases.
  \begin{compactitem}
  \item SQL Databases
  \item Online Transaction Processing
  \item Spreadsheet
  \item Online forms
  \item Sensors GPS and RFID
  \item Network and Web server logs
  \end{compactitem}
\item Semi-structured data, that is data which is partially organized and partially free-form.
  \begin{compactitem}
  \item E-mails
  \item XML and other markup language
  \item Binary executables
  \item TCP/IP packets
  \item Zipped files
  \item Integration of data
  \end{compactitem}
\item Unstructured data, that is data which can not be organized conventionally into rows and columns.
  \begin{compactitem}
  \item Web pages
  \item Social media feeds
  \item Images in varied file formats
  \item Video and Audio files
  \item Documents and PDF files
  \item Powerpoint presentation
  \item Media logs
  \item Surveys
  \end{compactitem}
\end{compactitem}

Data comes in a wide-ranging variety of file formats, such as, delimited text files, spreadsheets, XML, PDF, and JSON, each with its own list of benefits and limitations of use.

Data is extracted from multiple data sources, ranging from relational and non-relational databases, to APIs, web services, data streams, social platforms, and sensor devices.

Once the data is identified and gathered from different sources, it needs to be staged in a data repository so that it can be prepared for analysis. The type, format, and sources of data influence the type of data repository that can be used.

Data professionals need a host of languages that can help them extract, prepare, and analyse data. These can be classified as:
\begin{compactitem}
\item Querying languages, such as SQL, used for accessing and manipulating data from databases.
\item Programming languages such as Python, R, and Java, for developing applications and controlling application behavior.
\item Shell and Scripting languages, such as Unix/Linux Shell, and PowerShell, for automating repetitive operational tasks.
\end{compactitem}
```

## Data Repository

```{=latex}
A \textbf{Data Repository} is a general term that refers to data that has been collected, organized, and isolated so that it can be used for reporting, analytics, and also for archival purposes. 

The different types of Data Repositories include:
\begin{itemize}
\item Databases, which can be relational or non-relational, each following a set of organizational principles, the types of data they can store, and the tools that can be used to query, organize, and retrieve data.
\item Data Warehouses, that consolidate incoming data into one comprehensive store house. 
\item Data Marts, that are essentially sub-sections of a data warehouse, built to isolate data for a particular business function or use case.
\item Data Lakes, that serve as storage repositories for large amounts of structured, semi-structured, and unstructured data in their native format.
\item Big Data Stores, that provide distributed computational and storage infrastructure to store, scale, and process very large data sets.
\end{itemize}

The ETL, or Extract Transform and Load, Process is an automated process that converts raw data into analysis-ready data by:
\begin{compactitem}
\item Extracting data from source locations.
\item Transforming raw data by cleaning, enriching, standardizing, and validating it.
\item Loading the processed data into a destination system or data repository.
\end{compactitem}

The ELT, or Extract Load and Transfer, Process is a variation of the ETL Process. In this process, extracted data is loaded into the target system before the transformations are applied. This process is ideal for Data Lakes and working with Big Data.

Data Pipeline, sometimes used interchangeably with ETL and ELT, encompasses the entire journey of moving data from its source to a destination data lake or application, using the ETL or ELT process.

Data Integration Platforms combine disparate sources of data, physically or logically, to provide a unified view of the data for analytics purposes.
```

## Big Data Platforms

```{=latex}
Big Data refers to the vast amounts of data that is being produced each moment of every day, by people, tools, and machines. The sheer velocity, volume, and variety of data challenged the tools and systems used for conventional data, leading to the emergence of processing tools and platforms designed specifically for Big Data.

\textbf{V's of Big Data}
\begin{compactitem}
\item Velocity - speed at which data accumulates. Data is being generated extremely fast, in a process that never stops. Near or real-time streaming, local, and cloud-based technologies can process information very quickly. 
\item Volume - scale of the data, or the increase in the amount of data stored. 
\item Variety - the diversity of the data. Structured data fits neatly into rows and columns, in relational databases while unstructured data is not organized in a pre-defined way, like Tweets, blog posts, pictures, numbers, and video. Variety also reflects that data comes from different sources, machines, people, and processes, both internal and external to organizations. Drivers are mobile technologies, social media, wearable technologies, geo technologies, video, and many, many more. 
\item Veracity - the quality and origin of data, and its conformity to facts and accuracy. Attributes include consistency, completeness, integrity, and ambiguity. 
\item Value - our ability and need to turn data into value. \end{compactitem}

Big Data processing technologies help derive value from big data. These include NoSQL databases and Data Lakes and open-source technologies such as Apache Hadoop, Apache Hive, and Apache Spark.
\begin{compactitem}
\item Apache Hadoop provides distributed storage and processing of large datasets across clusters of computers. One of its main components, the Hadoop File Distribution System, or HDFS, is a storage system for big data.
\item Apache Hive is a data warehouse software for reading, writing, and managing large datasets files that are stored directly in either HDFS or other data storage systems such as Apache HBase.
\item Apache Spark is a general-purpose data processing engine designed to extract and process large volumes of data. It is used to perform complex analytics in real-time.
\end{compactitem}

```

## Data Platform Layer

```{=latex}
The architecture of a data platform can be seen as a set of layers, or functional components, each one performing a set of specific tasks. These layers include:
\begin{itemize}
\item \textbf{Data Ingestion Layer:} Responsible for connecting to source systems and bringing data into the platform. It transfers data in streaming or batch modes and maintains metadata about the collected data.
\item \textbf{Data Storage and Integration Layer:} Stores data for processing, transforms and merges data logically or physically, and makes data available for processing in streaming and batch modes. Relational and non-relational databases are commonly used in this layer.
\item \textbf{Data Processing Layer:} Processes data by performing validations, transformations, and applying business logic. It reads data from storage, supports querying tools and programming languages, and allows analysts and data scientists to work with the data.
\item \textbf{Analysis and User Interface Layer:} Delivers processed data to various users, including business intelligence analysts, data scientists, and other applications. It supports querying tools, programming languages, APIs, and visualization tools like dashboards and business intelligence applications.
\item \textbf{Data Pipeline Layer:} Overlaying other layers, it implements and maintains a continuously flowing data pipeline. It uses Extract, Transform, and Load (ETL) tools such as Apache Airflow and DataFlow to ensure a smooth data flow across the platform.
\end{itemize}

A well-designed data repository is essential for building a system that is scalable and capable of performing during high workloads. 

The choice or design of a data store is influenced by the type and volume of data that needs to be stored, the intended use of data, and storage considerations. The privacy, security, and governance needs of your organization also influence this choice.

The CIA, or Confidentiality, Integrity, and Availability triad are three key components of an effective strategy for information security. The CIA triad is applicable to all facets of security, be it infrastructure, network, application, or data security.
```

## Data Collection and Wrangling

```{=latex}
Depending on where the data must be sourced from, there are a number of methods and tools available for gathering data. These include query languages for extracting data from databases, APIs, Web Scraping, Data Streams, RSS Feeds, and Data Exchanges. 

Once the data you need has been gathered and imported, your next step is to make it analytics-ready. This is where the process of Data Wrangling, or Data Munging, comes in. 

Data Wrangling involves a whole range of transformations and cleansing activities performed on the data. Transformation of raw data includes the tasks you undertake to: 
\begin{compactitem}
\item Structurally manipulate and combine data using Joins and Unions. 
\item Normalize data, that is, clean the database of unused and redundant data.
\item Denormalize data, that is, combine data from multiple tables into a single table so that it can be queried faster.
\end{compactitem}

Cleansing activities include: 
\begin{compactitem}
\item Profiling data to uncover anomalies and quality issues.
\item Visualizing data using statistical methods in order to spot outliers. 
\item Fixing issues such as missing values, duplicate data, irrelevant data, inconsistent formats, syntax errors, and outliers. 
\end{compactitem}

A variety of software and tools are available for the data wrangling process. Some of the popularly used ones include Excel Power Query, Spreadsheets, OpenRefine, Google DataPrep, Watson Studio Refinery, Trifacta Wrangler, Python, and R, each with their own set of features, strengths, limitations, and applications.
```

## Querying Data, Performance Tuning and Troubleshooting
```{=latex}
\begin{compactitem}
\item In order for raw data to become analytics-ready, a number of transformation and cleansing tasks need to be performed on raw data. And that requires you to understand your dataset from multiple perspectives. One of the ways in which you can explore your dataset is to query it. 
\item Basic querying techniques can help you explore your data, such as, counting and aggregating a dataset, identifying extreme values, slicing data, sorting data, filtering patterns, and grouping data.
\item In a data engineering lifecycle, the performance of data pipelines, platforms, databases, applications, tools, queries, and scheduled jobs, need to be constantly monitored for performance and availability. 
\item The performance of a data pipeline can get impacted if the workload increases significantly, or there are application failures, or a scheduled job does not work as expected, or some of the tools in the pipeline run into compatibility issues. 
\item Databases are susceptible to outages, capacity overutilization, application slowdown, and conflicting activities and queries being executed simultaneously. 
\item Monitoring and alerting systems collect quantitative data in real time to give visibility into the performance of data pipelines, platforms, databases, applications, tools, queries, scheduled jobs, and more.
\item Time-based and condition-based maintenance schedules generate data that helps identify systems and procedures responsible for faults and low availability.
\end{compactitem}
```

## Career Opportunities in Data Engineering
```{=latex}
Data Engineering is reported to be one of the top ten jobs experiencing tremendous growth in the U.S. today. It is also reported to be one of the fastest growing tech occupations with year-over-year growth of around 50%. 

Currently, the demand for skilled data engineers far outweighs the supply, which means companies are willing to pay a premium to hire skilled data engineers.

Data engineering roles in organizations tend to break the specialization up into Data Architecture, Database Design and Architecture, Data Platforms, Data Pipelines and ETL, Data Warehouses, and Big Data.

Regardless of the niche you choose to specialize in, knowledge of operating systems, languages, databases, and infrastructure components, is essential. 

To work your way up from a Junior Data Engineer to a Lead or Principal Data Engineer, you need to continually advance your technical, functional, and soft skills from a foundational level to an expert level. You need to not only expand your skills in your niche area, but also into other areas of data engineering at the same time.

Big Data Engineers and Machine Learning Engineers are some of the emerging roles in this field and they require specialized skills in addition to basic data engineering.

There are several paths you can consider in order to gain entry into the data engineering field. 
\begin{compactitem}
\item An academic degree in Computer Science or engineering qualifies you for an entry-level job.
\item If you are not a graduate, or a graduate in a non-relate stream, you can earn professional certifications from online multi-course specializations offered by learning platforms such as Coursera, edX, and Udacity.
\item If you have a coding background, or you are an IT Support Specialist, a Software Tester, a Programmer, or a data professional such as a Statistician, Data Analyst, or BI Analyst, you can upskill with the help of online courses to become a Data Engineer. 
\end{compactitem}

```


